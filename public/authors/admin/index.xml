<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peyman Kor</title>
    <link>/authors/admin/</link>
      <atom:link href="/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Peyman Kor</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 05 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Peyman Kor</title>
      <link>/authors/admin/</link>
    </image>
    
    <item>
      <title>When your Docker Meets Pyspark to Do Sentiment Analysis of 10&#43; GB Customer Review Data-PART 1</title>
      <link>/post/pyspark_docker/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/pyspark_docker/</guid>
      <description>&lt;p&gt;In this blog I going to show you how you could easily use the Pyspark to wrangle the gigabyte scale data set. OK, so let&amp;rsquo;s get started:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;start.jpeg&#39;, width=1000, height=1000)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_3_0.jpg&#34; alt=&#34;jpeg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;setting-up-the-docker-engine&#34;&gt;Setting up the Docker Engine&lt;/h2&gt;
&lt;p&gt;Now, here the docker will be used to easily download the jupyter/pyspark docker image and then use it for distributed processing. So, the first thing you must know is whether your OS has a Docker engine.&lt;/p&gt;
&lt;p&gt;The Linux user will not have trouble with this one  and they can simply follow the instruction to set-up the docker in their OS from the link:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/ubuntu/&#34;&gt;Docker Manual for Linux Users:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For Windows and Mac user you can follow the official link to set-up your docker engine:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/docker-for-windows/install/&#34;&gt;Docker Manual Windows Users:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/docker-for-mac/install/&#34;&gt;Docker Manual Mac Users:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note: if you are a Data Scientist/ Analyst reading this post, I &lt;em&gt;highly&lt;/em&gt; recommend you to work with Linux OS distribution since it will really help you especially when it comes putting the Data Science results to the production:&lt;/p&gt;
&lt;p&gt;Now, having the Docker engine, the next thing we must do is to get the pyspark image (if you do not have it). This can be easily done through the following command in your bash:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull jupyter/pyspark-notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a bit a large file (around 4.5GB), after pulling we need to double check we have image
using the command line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~/Documents/pyspark_docker$ sudo docker image ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the list of all images in our local machine, we can see that
the &lt;em&gt;jupyter/pyspark-notebook&lt;/em&gt; is among the images that we will utilize it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~/Documents/pyspark_docker$ sudo docker image ls --all
REPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE
jupyter/pyspark-notebook   latest              5019fd934efa        2 weeks ago         4.4GB
jupyter/minimal-notebook   latest              bd466ef7da5f        2 weeks ago         2.52GB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if you have the jupyter/pyspark-notebook on your list, GREAT!.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;success.jpg&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_14_0.jpg&#34; alt=&#34;jpeg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;port-local-directory-to-the-docker-container&#34;&gt;Port local Directory to the Docker Container&lt;/h2&gt;
&lt;p&gt;Now you have a image of spark to wrangle the big data.So now since most of the time our big data is not in the same directory the docker is, we need to port the big data set to the container, so the container have direct access to the data, in my case the following code make this mounting (I will break it in the follow):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~$ sudo docker run -p 8888:8888 -v ~/Documents/pyspark_docker:/home/jovyan jupyter/pyspark-notebook

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, let&amp;rsquo;s break the above code to fully understand it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~$ sudo docker run -p 8888:8888
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this one pass traffic from port 8888 on our machine into port 8888 on the Docker image,
in this case (jupyter/pyspark-notebook)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;-v ~/Documents/pyspark_docker:/home/jovyan jupyter/pyspark-notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, replace &amp;ldquo;~/Documents/pyspark_docker&amp;rdquo; with your local working directory. This directory will be accessed by the container, thatâ€™s what option &amp;ldquo;-v&amp;rdquo; is doing at the code. The directory might be empty, you will need to put some files later.
So if you done the above steps, now the Jupyter notebook should comes up in your browser on the exact path you will have your data. Now, if you have reached this stage, CONGRATULATION, now you are ready to work with the big data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;sweet.jpeg&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_23_0.jpg&#34; alt=&#34;jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this work the yelp data set will be used for distributed computing with spark. The Yelp data set available at this link will be used as typical business big data:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.yelp.com/dataset&#34;&gt;Open Source Link for Yelp Dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For this particular data, I found this blog quite helpful for data modeling of the data, as could be shown
in the below:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://paulx-cn.github.io/blog/6th_Blog/&#34;&gt;Data Modeling&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;yelpdatamodel.png&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;start-data-wrangling-with-spark-session&#34;&gt;Start Data Wrangling with Spark Session&lt;/h2&gt;
&lt;h3 id=&#34;set-up-the-pyspark&#34;&gt;Set up the Pyspark&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pyspark
from pyspark.sql.types import FloatType
from pyspark.sql.types import StringType
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName(&amp;quot;Big Data Wrangling with Pyspark&amp;quot;) \
    .config(&amp;quot;spark.some.config.option&amp;quot;, &amp;quot;some-value&amp;quot;) \
    .getOrCreate()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;read-the-review-data-through-the-spark&#34;&gt;Read the Review data through the Spark&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_review = spark.read.json(&amp;quot;yelp_academic_dataset_review.json&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just having look on the size of the data, we have around 80 million review, indeed a big data!:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Data description
#Checking Attributes and Rows
print(&#39;number of rows:&#39;+ str(df_review.count()))
print(&#39;number of columns:&#39;+ str(len(df_review.columns)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;number of rows:8021122
number of columns:9
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;sparkeasy.png&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_review.printSchema()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;root
 |-- business_id: string (nullable = true)
 |-- cool: long (nullable = true)
 |-- date: string (nullable = true)
 |-- funny: long (nullable = true)
 |-- review_id: string (nullable = true)
 |-- stars: double (nullable = true)
 |-- text: string (nullable = true)
 |-- useful: long (nullable = true)
 |-- user_id: string (nullable = true)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;In the below code, I am defining the &lt;em&gt;text_processing&lt;/em&gt; function which will remove the punctuation, make all reviews lower case and remove as well English stop words:&lt;/p&gt;
&lt;p&gt;In this step you may need to run the following code to install the NLTK package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!pip install NLTK
#!pip install afinn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.feature_extraction import stop_words
import string
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.
  warnings.warn(message, FutureWarning)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql.functions import udf

def text_processing(text):
    # remove punctuation 
    text = &amp;quot;&amp;quot;.join([c for c in text 
                    if c not in string.punctuation])
    # lowercase
    text = &amp;quot;&amp;quot;.join([c.lower() for c in text])
    # remove stopwords
    text = &amp;quot; &amp;quot;.join([w for w in text.split() 
                     if w not in stop_words.ENGLISH_STOP_WORDS])
    # stemming / lematizing (optional)
    text = &amp;quot; &amp;quot;.join([lemmatizer.lemmatize(w) for w in text.split()])
    return text

text_processing_udf = udf(text_processing, StringType())
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;adding-the-clean_text-column&#34;&gt;Adding the clean_text Column&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df_review.withColumn(&amp;quot;clean_text&amp;quot;, text_processing_udf(df_review[&#39;text&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.select([&amp;quot;clean_text&amp;quot;, &amp;quot;text&amp;quot;]).show(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+--------------------+--------------------+
|          clean_text|                text|
+--------------------+--------------------+
|worked museum eag...|As someone who ha...|
|actually horrifie...|I am actually hor...|
|love deagans real...|I love Deagan&#39;s. ...|
|dismal lukewarm d...|Dismal, lukewarm,...|
|oh happy day fina...|Oh happy day, fin...|
+--------------------+--------------------+
only showing top 5 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, now we have the clean text, it is time to do the sentiment analysis to see the score of the each review&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import afinn
from afinn import Afinn
afinn = Afinn()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sentiment(text):
    score_sentiment=afinn.score(text)
    return score_sentiment

sentiment_processing_udf = udf(sentiment, FloatType())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment = df.withColumn(&amp;quot;score&amp;quot;, sentiment_processing_udf(df[&#39;clean_text&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment.select([&#39;score&#39;,&#39;clean_text&#39;]).show(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+-----+--------------------+
|score|          clean_text|
+-----+--------------------+
| 21.0|worked museum eag...|
|-11.0|actually horrifie...|
| 13.0|love deagans real...|
| -7.0|dismal lukewarm d...|
| 26.0|oh happy day fina...|
+-----+--------------------+
only showing top 5 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment.printSchema()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;root
 |-- business_id: string (nullable = true)
 |-- cool: long (nullable = true)
 |-- date: string (nullable = true)
 |-- funny: long (nullable = true)
 |-- review_id: string (nullable = true)
 |-- stars: double (nullable = true)
 |-- text: string (nullable = true)
 |-- useful: long (nullable = true)
 |-- user_id: string (nullable = true)
 |-- clean_text: string (nullable = true)
 |-- score: float (nullable = true)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;grouping-business-based-on-their-review-score&#34;&gt;Grouping Business Based on their Review Score&lt;/h2&gt;
&lt;p&gt;Here, I am grouping the mean score of the all businesses:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment_grouped=df_sentiment.select([&amp;quot;business_id&amp;quot;,&#39;score&#39;]).groupby([&#39;business_id&#39;]).mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment_grouped.printSchema()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;root
 |-- business_id: string (nullable = true)
 |-- avg(score): double (nullable = true)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment_grouped.count()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;209393
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
