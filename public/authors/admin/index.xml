<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peyman Kor</title>
    <link>/authors/admin/</link>
      <atom:link href="/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Peyman Kor</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 09 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Peyman Kor</title>
      <link>/authors/admin/</link>
    </image>
    
    <item>
      <title>How to Reproduce Financial Times Style COVID19 Daily Reporting?</title>
      <link>/post/ft/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/ft/</guid>
      <description>


&lt;p&gt;Journalist of the &lt;a href=&#34;http://www.ft.com/&#34;&gt;Financial Times&lt;/a&gt;, &lt;a href=&#34;https://www.ft.com/john-burn-murdoch&#34;&gt;John Burn-Murdoch&lt;/a&gt; became quite well known during pandemic time with his I would call it “Enlightening” daily reporting of the evolution of the COVID19 around the world. I felt that it would be a good practice if someone could write blog on how to “produce” these plots because essentially, these plots can be used in many cases for example, you have the dataset of the three dimensional where the third dimension is categorical, and you want the compares the categories.&lt;/p&gt;
&lt;p&gt;Here is the particular plot that I “try” to reproduce it,&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Small multiples for daily new deaths in 54 countries:&lt;br&gt;• Norway locked down while Sweden didn’t; Norway’s daily death toll rising much more slowly than Sweden’s&lt;br&gt;• Australia faring well so far&lt;br&gt;• In Europe, Austria &amp;amp; Denmark faring well&lt;br&gt;&lt;br&gt;All charts: &lt;a href=&#34;https://t.co/JxVd2cG7KI&#34;&gt;https://t.co/JxVd2cG7KI&lt;/a&gt; &lt;a href=&#34;https://t.co/JulZYu5VJo&#34;&gt;pic.twitter.com/JulZYu5VJo&lt;/a&gt;&lt;/p&gt;&amp;mdash; John Burn-Murdoch (@jburnmurdoch) &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1250538655707430913?ref_src=twsrc%5Etfw&#34;&gt;April 15, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Allright this is the visualization of the Daily Death tolls (7-day moving average) VS. the days since average daily deaths passed the 3.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s go started and see how we could reproduce such a nice visualization!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1280/1*8xHK79BmxxbYpjCR20Vtbg.jpeg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Libraries&lt;/h2&gt;
&lt;p&gt;Here I am using varieties of the libraries both for data wrangling and as well for visualization, but you could say 90% of the tools are available in the framework of the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;Tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I wrote the small comment in front of th each package so you may know it better where I may use them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)                                        # main package
library(zoo)                                              # was used for moving average
#devtools::install_github(&amp;quot;yutannihilation/gghighlight&amp;quot;)   # for highlight specif plot at each facet  
library(gghighlight)                                      # 
#install.packages(&amp;#39;ggthemes&amp;#39;)                             # if you have not installed!
library(ggthemes)                                         # may be needed for specif need
#remotes::install_github(&amp;quot;Financial-Times/ftplottools&amp;quot;)    #
library(ftplottools)                                      #
library(coronavirus)                                      # used for corona dataset&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Wrangling&lt;/h2&gt;
&lt;p&gt;Here we load the &lt;code&gt;coronavirus&lt;/code&gt; dataset and we could have brief look on that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;coronavirus&amp;quot;)
coronavirus %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# # A tibble: 6 x 7
#   Province.State Country.Region   Lat  Long date       cases type     
#   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    
# 1 &amp;quot;&amp;quot;             Afghanistan       33    65 2020-01-22     0 confirmed
# 2 &amp;quot;&amp;quot;             Afghanistan       33    65 2020-01-23     0 confirmed
# 3 &amp;quot;&amp;quot;             Afghanistan       33    65 2020-01-24     0 confirmed
# 4 &amp;quot;&amp;quot;             Afghanistan       33    65 2020-01-25     0 confirmed
# 5 &amp;quot;&amp;quot;             Afghanistan       33    65 2020-01-26     0 confirmed
# 6 &amp;quot;&amp;quot;             Afghanistan       33    65 2020-01-27     0 confirmed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, the first thing we should do is that, I am going to select and work on the only countries that have been presented in our first plot, so I manually extracted those countries (I hate such a manual process!):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selected_cou_FT &amp;lt;- c(&amp;quot;Algeria&amp;quot;, &amp;quot;Argentina&amp;quot;,&amp;quot;Australia&amp;quot;, &amp;quot;Austria&amp;quot;,
&amp;quot;Bangladesh&amp;quot;,&amp;quot;Belgium&amp;quot;,&amp;quot;Bosnia and Herzegovina&amp;quot;,
&amp;quot;Brazil&amp;quot;,&amp;quot;Canada&amp;quot;,&amp;quot;Chile&amp;quot;,&amp;quot;China&amp;quot;,&amp;quot;Colombia&amp;quot;,&amp;quot;Czechia&amp;quot;, &amp;quot;Denmark&amp;quot;,
&amp;quot;Dominican Republic&amp;quot;,&amp;quot;Egypt&amp;quot;,&amp;quot;Finland&amp;quot;,&amp;quot;France&amp;quot;,&amp;quot;Germany&amp;quot;,&amp;quot;Greece&amp;quot;,&amp;quot;Hungary&amp;quot;,
&amp;quot;India&amp;quot;,&amp;quot;Indonesia&amp;quot;,&amp;quot;Iran&amp;quot;,&amp;quot;Iraq&amp;quot;,&amp;quot;Ireland&amp;quot;,&amp;quot;Israel&amp;quot;,&amp;quot;Italy&amp;quot;,&amp;quot;Japan&amp;quot;,&amp;quot;Malaysia&amp;quot;,
&amp;quot;Mexico&amp;quot;,&amp;quot;Moldova&amp;quot;,&amp;quot;Morocco&amp;quot;,&amp;quot;Netherlands&amp;quot;,&amp;quot;Norway&amp;quot;,&amp;quot;Pakistan&amp;quot;,&amp;quot;Panama&amp;quot;,&amp;quot;Peru&amp;quot;,
&amp;quot;Philippines&amp;quot;,&amp;quot;Poland&amp;quot;,&amp;quot;Portugal&amp;quot;,&amp;quot;Russia&amp;quot;,&amp;quot;Romania&amp;quot;,&amp;quot;Korea, South&amp;quot;,&amp;quot;Saudi Arabia&amp;quot;, 
&amp;quot;Serbia&amp;quot;, &amp;quot;Spain&amp;quot;,&amp;quot;Sweden&amp;quot;, &amp;quot;Switzerland&amp;quot;,&amp;quot;Turkey&amp;quot;,&amp;quot;United Kingdom&amp;quot;, &amp;quot;Ukraine&amp;quot;,&amp;quot;US&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, now two important manuplation in the data must be done:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;we need to sum all death case grouped by date and country region, since here we have additional rows that represent death in the sub region, so all of the must be summed up to one country.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then the moving average of the death cases (7-days) are computed. Here, for initial first three days we have &lt;code&gt;NA&lt;/code&gt; for the moving average since at moving average calculation, we need the three days before and after the middle point.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then since the plot of the FT says days after &lt;code&gt;daily deaths exceeded the 3 cases&lt;/code&gt; we sliced the data from the day death&amp;gt;3…&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the end I am still feel there is some difference in my approach for calculation of the numbers with the John Burn-Murdoch, since although the processed data are similar, yet seems minor difference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;org_sel_con_movavg &amp;lt;- coronavirus %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  filter(Country.Region %in% selected_cou_FT) %&amp;gt;% 
  filter(date &amp;lt;= as.Date(&amp;#39;2020-04-15&amp;#39;)) %&amp;gt;% 
  group_by(date, Country.Region) %&amp;gt;%
  filter(type==&amp;quot;death&amp;quot;) %&amp;gt;% 
  summarise(total_deth=sum(cases)) %&amp;gt;%
  group_by(Country.Region) %&amp;gt;% 
  mutate(week_movavg=rollmean(total_deth, k = 7, fill = &amp;quot;extend&amp;quot;)) %&amp;gt;% 
  slice(match(TRUE, total_deth&amp;gt;3):n()) %&amp;gt;% 
  mutate(id = row_number())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visulization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visulization&lt;/h2&gt;
&lt;p&gt;Allright, now first try the basic visualization using the &lt;code&gt;facet_wrap&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(org_sel_con_movavg, aes(id, week_movavg, color=Country.Region)) +
  geom_path(color=&amp;#39;blue&amp;#39;, lineend = &amp;quot;round&amp;quot;, size=2) +
  gghighlight() +
  facet_wrap(~ Country.Region, nrow = 8, scales = &amp;quot;free_x&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/FT/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, not bad for first try :) . So here, now I am going to &lt;code&gt;polish&lt;/code&gt; the plot, so will be adding the theme component. The comprehensive guide on what is the affect of each component in theme is on the plot are available in &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/ggtheme.html&#34;&gt;tidyverse website&lt;/a&gt; I wrote a small comment next to the each change, but you can change it depending on your selection:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mytheme &amp;lt;- 
  theme(panel.grid.major.x =element_line(colour = &amp;quot;wheat4&amp;quot;),
        panel.grid.major.y =element_line(colour = &amp;quot;wheat4&amp;quot;),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text = element_text(colour = &amp;quot;black&amp;quot;, size = 24, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_text(colour = &amp;quot;black&amp;quot;, size = 28, face = &amp;quot;bold&amp;quot;, vjust = 0.8),
        axis.title.y = element_blank(),
        strip.text = element_text(size=24, colour = &amp;quot;blue4&amp;quot;, face = &amp;quot;bold&amp;quot;),
        plot.title = element_text(color = &amp;quot;black&amp;quot;, size = 40, face = &amp;quot;bold&amp;quot;),
        plot.subtitle = element_text(color = &amp;quot;black&amp;quot;, size = 30),
        plot.tag = element_text(color = &amp;quot;black&amp;quot;, face = &amp;quot;italic&amp;quot;, size = 24, lineheight = 0.9),
        plot.tag.position = c(0.15,0.02),
        panel.background = element_rect(fill = &amp;quot;seashell2&amp;quot;),
        plot.background = element_rect(fill = &amp;quot;seashell2&amp;quot;), 
        panel.border = element_blank(),
        panel.spacing.y = unit(3, &amp;quot;lines&amp;quot;)
        ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, now our theme is ready but I should mention that finding the right theme is an &lt;code&gt;iterative&lt;/code&gt; process, I test and plot the visulization, and then come back on tune it. Now in the below, two points are important to be mentioned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I use the package &lt;code&gt;here&lt;/code&gt; which change my directory the the place this index.Rmd file resides&lt;/li&gt;
&lt;li&gt;I use the png(…) to save the file and write the outcome of the ggplot on this file.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;here::dr_here()
png(file=&amp;quot;testold.png&amp;quot;,
    width=2500, height=2225)

ggplot(org_sel_con_movavg, aes(id, week_movavg, color=Country.Region)) +
  geom_path(color=&amp;#39;blue&amp;#39;, lineend = &amp;quot;round&amp;quot;, size=2) +
  gghighlight() +
  facet_wrap(~ Country.Region, nrow = 8, scales = &amp;quot;free_x&amp;quot;) +
  scale_y_log10(limits=c(1,1000)) +
  scale_x_continuous(breaks = c(0,30,60,90)) +
  mytheme +
  labs(title=&amp;quot;\nDaily Death tolls&amp;quot;,
       x =&amp;quot;\n Days since average daily deaths passed 3 \n&amp;quot;, 
       subtitle = &amp;quot; \n Daily Death with Coronavisrus (7-day Rolling Average), by number of the days since 3 daily deaths first recorded \n&amp;quot;,
       tag = &amp;quot;Graphics: Peyman Kor | @peyman_kor  \n Source of data: coronavisus package developed by Rami Krispin&amp;quot;)
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# png 
#   2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK now the plot is saved and we can use the simple &lt;code&gt;knitr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;here::set_here()
knitr::include_graphics(&amp;quot;testold.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;testold.png&#34; width=&#34;1250&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At the end, I feel this is a good practice to have decent visualization for three dimensional data when your third dimension is categorical. My goal was not to make this plot for COVID19 case, but I find this template is useful for many dataset to compare the quantities in the different categories.&lt;/p&gt;
&lt;p&gt;If you have come along so far reading this post, thanks :) and you are welcome for feedback!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>When your Docker Meets Pyspark to Do Sentiment Analysis of 10&#43; GB Customer Review Data-PART 1</title>
      <link>/post/pyspark_docker/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/pyspark_docker/</guid>
      <description>&lt;p&gt;In this blog I going to show you how you could easily use the Pyspark to wrangle the gigabyte scale data set. OK, so let&amp;rsquo;s get started:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;start.jpeg&#39;, width=1000, height=1000)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_3_0.jpg&#34; alt=&#34;jpeg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;setting-up-the-docker-engine&#34;&gt;Setting up the Docker Engine&lt;/h2&gt;
&lt;p&gt;Now, here the docker will be used to easily download the jupyter/pyspark docker image and then use it for distributed processing. So, the first thing you must know is whether your OS has a Docker engine.&lt;/p&gt;
&lt;p&gt;The Linux user will not have trouble with this one  and they can simply follow the instruction to set-up the docker in their OS from the link:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/ubuntu/&#34;&gt;Docker Manual for Linux Users:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For Windows and Mac user you can follow the official link to set-up your docker engine:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/docker-for-windows/install/&#34;&gt;Docker Manual Windows Users:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/docker-for-mac/install/&#34;&gt;Docker Manual Mac Users:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note: if you are a Data Scientist/ Analyst reading this post, I &lt;em&gt;highly&lt;/em&gt; recommend you to work with Linux OS distribution since it will really help you especially when it comes putting the Data Science results to the production:&lt;/p&gt;
&lt;p&gt;Now, having the Docker engine, the next thing we must do is to get the pyspark image (if you do not have it). This can be easily done through the following command in your bash:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull jupyter/pyspark-notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a bit a large file (around 4.5GB), after pulling we need to double check we have image
using the command line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~/Documents/pyspark_docker$ sudo docker image ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the list of all images in our local machine, we can see that
the &lt;em&gt;jupyter/pyspark-notebook&lt;/em&gt; is among the images that we will utilize it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~/Documents/pyspark_docker$ sudo docker image ls --all
REPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE
jupyter/pyspark-notebook   latest              5019fd934efa        2 weeks ago         4.4GB
jupyter/minimal-notebook   latest              bd466ef7da5f        2 weeks ago         2.52GB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if you have the jupyter/pyspark-notebook on your list, GREAT!.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;success.jpg&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_14_0.jpg&#34; alt=&#34;jpeg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;port-local-directory-to-the-docker-container&#34;&gt;Port local Directory to the Docker Container&lt;/h2&gt;
&lt;p&gt;Now you have a image of spark to wrangle the big data.So now since most of the time our big data is not in the same directory the docker is, we need to port the big data set to the container, so the container have direct access to the data, in my case the following code make this mounting (I will break it in the follow):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~$ sudo docker run -p 8888:8888 -v ~/Documents/pyspark_docker:/home/jovyan jupyter/pyspark-notebook

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, let&amp;rsquo;s break the above code to fully understand it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~$ sudo docker run -p 8888:8888
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this one pass traffic from port 8888 on our machine into port 8888 on the Docker image,
in this case (jupyter/pyspark-notebook)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;-v ~/Documents/pyspark_docker:/home/jovyan jupyter/pyspark-notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, replace &amp;ldquo;~/Documents/pyspark_docker&amp;rdquo; with your local working directory. This directory will be accessed by the container, that’s what option &amp;ldquo;-v&amp;rdquo; is doing at the code. The directory might be empty, you will need to put some files later.
So if you done the above steps, now the Jupyter notebook should comes up in your browser on the exact path you will have your data. Now, if you have reached this stage, CONGRATULATION, now you are ready to work with the big data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;sweet.jpeg&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_23_0.jpg&#34; alt=&#34;jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this work the yelp data set will be used for distributed computing with spark. The Yelp data set available at this link will be used as typical business big data:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.yelp.com/dataset&#34;&gt;Open Source Link for Yelp Dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For this particular data, I found this blog quite helpful for data modeling of the data, as could be shown
in the below:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://paulx-cn.github.io/blog/6th_Blog/&#34;&gt;Data Modeling&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;yelpdatamodel.png&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;start-data-wrangling-with-spark-session&#34;&gt;Start Data Wrangling with Spark Session&lt;/h2&gt;
&lt;h3 id=&#34;set-up-the-pyspark&#34;&gt;Set up the Pyspark&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pyspark
from pyspark.sql.types import FloatType
from pyspark.sql.types import StringType
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName(&amp;quot;Big Data Wrangling with Pyspark&amp;quot;) \
    .config(&amp;quot;spark.some.config.option&amp;quot;, &amp;quot;some-value&amp;quot;) \
    .getOrCreate()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;read-the-review-data-through-the-spark&#34;&gt;Read the Review data through the Spark&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_review = spark.read.json(&amp;quot;yelp_academic_dataset_review.json&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just having look on the size of the data, we have around 80 million review, indeed a big data!:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Data description
#Checking Attributes and Rows
print(&#39;number of rows:&#39;+ str(df_review.count()))
print(&#39;number of columns:&#39;+ str(len(df_review.columns)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;number of rows:8021122
number of columns:9
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;sparkeasy.png&#39;, width=1000, height=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./docker_pyspark_project_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_review.printSchema()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;root
 |-- business_id: string (nullable = true)
 |-- cool: long (nullable = true)
 |-- date: string (nullable = true)
 |-- funny: long (nullable = true)
 |-- review_id: string (nullable = true)
 |-- stars: double (nullable = true)
 |-- text: string (nullable = true)
 |-- useful: long (nullable = true)
 |-- user_id: string (nullable = true)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;In the below code, I am defining the &lt;em&gt;text_processing&lt;/em&gt; function which will remove the punctuation, make all reviews lower case and remove as well English stop words:&lt;/p&gt;
&lt;p&gt;In this step you may need to run the following code to install the NLTK package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!pip install NLTK
#!pip install afinn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.feature_extraction import stop_words
import string
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.
  warnings.warn(message, FutureWarning)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql.functions import udf

def text_processing(text):
    # remove punctuation 
    text = &amp;quot;&amp;quot;.join([c for c in text 
                    if c not in string.punctuation])
    # lowercase
    text = &amp;quot;&amp;quot;.join([c.lower() for c in text])
    # remove stopwords
    text = &amp;quot; &amp;quot;.join([w for w in text.split() 
                     if w not in stop_words.ENGLISH_STOP_WORDS])
    # stemming / lematizing (optional)
    text = &amp;quot; &amp;quot;.join([lemmatizer.lemmatize(w) for w in text.split()])
    return text

text_processing_udf = udf(text_processing, StringType())
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;adding-the-clean_text-column&#34;&gt;Adding the clean_text Column&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df_review.withColumn(&amp;quot;clean_text&amp;quot;, text_processing_udf(df_review[&#39;text&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.select([&amp;quot;clean_text&amp;quot;, &amp;quot;text&amp;quot;]).show(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+--------------------+--------------------+
|          clean_text|                text|
+--------------------+--------------------+
|worked museum eag...|As someone who ha...|
|actually horrifie...|I am actually hor...|
|love deagans real...|I love Deagan&#39;s. ...|
|dismal lukewarm d...|Dismal, lukewarm,...|
|oh happy day fina...|Oh happy day, fin...|
+--------------------+--------------------+
only showing top 5 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, now we have the clean text, it is time to do the sentiment analysis to see the score of the each review&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import afinn
from afinn import Afinn
afinn = Afinn()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sentiment(text):
    score_sentiment=afinn.score(text)
    return score_sentiment

sentiment_processing_udf = udf(sentiment, FloatType())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment = df.withColumn(&amp;quot;score&amp;quot;, sentiment_processing_udf(df[&#39;clean_text&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment.select([&#39;score&#39;,&#39;clean_text&#39;]).show(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+-----+--------------------+
|score|          clean_text|
+-----+--------------------+
| 21.0|worked museum eag...|
|-11.0|actually horrifie...|
| 13.0|love deagans real...|
| -7.0|dismal lukewarm d...|
| 26.0|oh happy day fina...|
+-----+--------------------+
only showing top 5 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment.printSchema()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;root
 |-- business_id: string (nullable = true)
 |-- cool: long (nullable = true)
 |-- date: string (nullable = true)
 |-- funny: long (nullable = true)
 |-- review_id: string (nullable = true)
 |-- stars: double (nullable = true)
 |-- text: string (nullable = true)
 |-- useful: long (nullable = true)
 |-- user_id: string (nullable = true)
 |-- clean_text: string (nullable = true)
 |-- score: float (nullable = true)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;grouping-business-based-on-their-review-score&#34;&gt;Grouping Business Based on their Review Score&lt;/h2&gt;
&lt;p&gt;Here, I am grouping the mean score of the all businesses:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment_grouped=df_sentiment.select([&amp;quot;business_id&amp;quot;,&#39;score&#39;]).groupby([&#39;business_id&#39;]).mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment_grouped.printSchema()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;root
 |-- business_id: string (nullable = true)
 |-- avg(score): double (nullable = true)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sentiment_grouped.count()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;209393
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
