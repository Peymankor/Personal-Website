[{"authors":["admin"],"categories":null,"content":"I am an M.sc degree student in Mathematical Modelling and Computer Science at the Technical University of Denmark and Data Scientist at a start-up company, Decarbonify. I am going to finish my degree in June 2020. In addition to my current role, I got my first master degree in Engineering from the University of Stavanger, Norway. For my first M.Sc degree, I worked on \u0026ldquo;Decision-Driven Data Analytics\u0026rdquo; with application in the Energy industry under the Supervision of Reidar B. Bratvold and Aojie Hong. The full text of the publication is available in the M.Sc thesis Website.\nMy area of interest is R and Python Programming for Descriptive and Predictive Analytics. In my Github portfolio I provided multiple of my data science projects written in R, Python and SQL query language. On the Deep Learning side, I did a project with Ole Winther on the \u0026ldquo;Bayesian Uncertainty quantification in Neural Networks\u0026rdquo; aiming to provide BayesBackprop as a new method for Regularization. The project was documented in this repo.\nIn the end, I find myself both a highly collaborative team player with an enthusiasm to work independently and a flexible attitude accustomed to working in an agile development environment. I enjoy brainstorming and discovering the business opportunities to be addressed through the data-driven approach.\n","date":1588636800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588723200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an M.sc degree student in Mathematical Modelling and Computer Science at the Technical University of Denmark and Data Scientist at a start-up company, Decarbonify. I am going to finish my degree in June 2020. In addition to my current role, I got my first master degree in Engineering from the University of Stavanger, Norway. For my first M.Sc degree, I worked on \u0026ldquo;Decision-Driven Data Analytics\u0026rdquo; with application in the Energy industry under the Supervision of Reidar B.","tags":null,"title":"Peyman Kor","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Peyman Kor"],"categories":[],"content":"In this blog I going to show you how you could easily use the Pyspark to wrangle the gigabyte scale data set. OK, so let\u0026rsquo;s get started:\nfrom IPython.core.display import Image Image('start.jpeg', width=1000, height=1000)  Setting up the Docker Engine Now, here the docker will be used to easily download the jupyter/pyspark docker image and then use it for distributed processing. So, the first thing you must know is whether your OS has a Docker engine.\nThe Linux user will not have trouble with this one and they can simply follow the instruction to set-up the docker in their OS from the link:\nDocker Manual for Linux Users:\nFor Windows and Mac user you can follow the official link to set-up your docker engine:\nDocker Manual Windows Users:\nDocker Manual Mac Users:\nNote: if you are a Data Scientist/ Analyst reading this post, I highly recommend you to work with Linux OS distribution since it will really help you especially when it comes putting the Data Science results to the production:\nNow, having the Docker engine, the next thing we must do is to get the pyspark image (if you do not have it). This can be easily done through the following command in your bash:\ndocker pull jupyter/pyspark-notebook  It is a bit a large file (around 4.5GB), after pulling we need to double check we have image using the command line:\n(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~/Documents/pyspark_docker$ sudo docker image ls  Here is the list of all images in our local machine, we can see that the jupyter/pyspark-notebook is among the images that we will utilize it:\n(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~/Documents/pyspark_docker$ sudo docker image ls --all REPOSITORY TAG IMAGE ID CREATED SIZE jupyter/pyspark-notebook latest 5019fd934efa 2 weeks ago 4.4GB jupyter/minimal-notebook latest bd466ef7da5f 2 weeks ago 2.52GB  Now, if you have the jupyter/pyspark-notebook on your list, GREAT!.\nfrom IPython.core.display import Image Image('success.jpg', width=1000, height=1000)  Port local Directory to the Docker Container Now you have a image of spark to wrangle the big data.So now since most of the time our big data is not in the same directory the docker is, we need to port the big data set to the container, so the container have direct access to the data, in my case the following code make this mounting (I will break it in the follow):\n(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~$ sudo docker run -p 8888:8888 -v ~/Documents/pyspark_docker:/home/jovyan jupyter/pyspark-notebook  OK, let\u0026rsquo;s break the above code to fully understand it:\n(base) peyman@peyman-ZenBook-UX433FN-UX433FN:~$ sudo docker run -p 8888:8888  So this one pass traffic from port 8888 on our machine into port 8888 on the Docker image, in this case (jupyter/pyspark-notebook)\n-v ~/Documents/pyspark_docker:/home/jovyan jupyter/pyspark-notebook  Here, replace \u0026ldquo;~/Documents/pyspark_docker\u0026rdquo; with your local working directory. This directory will be accessed by the container, that’s what option \u0026ldquo;-v\u0026rdquo; is doing at the code. The directory might be empty, you will need to put some files later. So if you done the above steps, now the Jupyter notebook should comes up in your browser on the exact path you will have your data. Now, if you have reached this stage, CONGRATULATION, now you are ready to work with the big data:\nfrom IPython.core.display import Image Image('sweet.jpeg', width=1000, height=1000)  In this work the yelp data set will be used for distributed computing with spark. The Yelp data set available at this link will be used as typical business big data:\nOpen Source Link for Yelp Dataset\nFor this particular data, I found this blog quite helpful for data modeling of the data, as could be shown in the below:\nData Modeling\nfrom IPython.core.display import Image Image('yelpdatamodel.png', width=1000, height=1000)  Start Data Wrangling with Spark Session Set up the Pyspark import pyspark from pyspark.sql.types import FloatType from pyspark.sql.types import StringType  from pyspark.sql import SparkSession spark = SparkSession \\ .builder \\ .appName(\u0026quot;Big Data Wrangling with Pyspark\u0026quot;) \\ .config(\u0026quot;spark.some.config.option\u0026quot;, \u0026quot;some-value\u0026quot;) \\ .getOrCreate()  Read the Review data through the Spark df_review = spark.read.json(\u0026quot;yelp_academic_dataset_review.json\u0026quot;)  Just having look on the size of the data, we have around 80 million review, indeed a big data!:\n#Data description #Checking Attributes and Rows print('number of rows:'+ str(df_review.count())) print('number of columns:'+ str(len(df_review.columns)))  number of rows:8021122 number of columns:9  from IPython.core.display import Image Image('sparkeasy.png', width=1000, height=1000)  df_review.printSchema()  root |-- business_id: string (nullable = true) |-- cool: long (nullable = true) |-- date: string (nullable = true) |-- funny: long (nullable = true) |-- review_id: string (nullable = true) |-- stars: double (nullable = true) |-- text: string (nullable = true) |-- useful: long (nullable = true) |-- user_id: string (nullable = true)  Sentiment Analysis In the below code, I am defining the text_processing function which will remove the punctuation, make all reviews lower case and remove as well English stop words:\nIn this step you may need to run the following code to install the NLTK package.\n#!pip install NLTK #!pip install afinn  from sklearn.feature_extraction import stop_words import string from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer()  /opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API. warnings.warn(message, FutureWarning)  from pyspark.sql.functions import udf def text_processing(text): # remove punctuation text = \u0026quot;\u0026quot;.join([c for c in text if c not in string.punctuation]) # lowercase text = \u0026quot;\u0026quot;.join([c.lower() for c in text]) # remove stopwords text = \u0026quot; \u0026quot;.join([w for w in text.split() if w not in stop_words.ENGLISH_STOP_WORDS]) # stemming / lematizing (optional) text = \u0026quot; \u0026quot;.join([lemmatizer.lemmatize(w) for w in text.split()]) return text text_processing_udf = udf(text_processing, StringType())  Adding the clean_text Column df = df_review.withColumn(\u0026quot;clean_text\u0026quot;, text_processing_udf(df_review['text']))  df.select([\u0026quot;clean_text\u0026quot;, \u0026quot;text\u0026quot;]).show(5)  +--------------------+--------------------+ | clean_text| text| +--------------------+--------------------+ |worked museum eag...|As someone who ha...| |actually horrifie...|I am actually hor...| |love deagans real...|I love Deagan's. ...| |dismal lukewarm d...|Dismal, lukewarm,...| |oh happy day fina...|Oh happy day, fin...| +--------------------+--------------------+ only showing top 5 rows  OK, now we have the clean text, it is time to do the sentiment analysis to see the score of the each review\nimport afinn from afinn import Afinn afinn = Afinn()  def sentiment(text): score_sentiment=afinn.score(text) return score_sentiment sentiment_processing_udf = udf(sentiment, FloatType())  df_sentiment = df.withColumn(\u0026quot;score\u0026quot;, sentiment_processing_udf(df['clean_text']))  df_sentiment.select(['score','clean_text']).show(5)  +-----+--------------------+ |score| clean_text| +-----+--------------------+ | 21.0|worked museum eag...| |-11.0|actually horrifie...| | 13.0|love deagans real...| | -7.0|dismal lukewarm d...| | 26.0|oh happy day fina...| +-----+--------------------+ only showing top 5 rows  df_sentiment.printSchema()  root |-- business_id: string (nullable = true) |-- cool: long (nullable = true) |-- date: string (nullable = true) |-- funny: long (nullable = true) |-- review_id: string (nullable = true) |-- stars: double (nullable = true) |-- text: string (nullable = true) |-- useful: long (nullable = true) |-- user_id: string (nullable = true) |-- clean_text: string (nullable = true) |-- score: float (nullable = true)  Grouping Business Based on their Review Score Here, I am grouping the mean score of the all businesses:\ndf_sentiment_grouped=df_sentiment.select([\u0026quot;business_id\u0026quot;,'score']).groupby(['business_id']).mean()  df_sentiment_grouped.printSchema()  root |-- business_id: string (nullable = true) |-- avg(score): double (nullable = true)  df_sentiment_grouped.count()  209393  ","date":1588636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2020-05-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"In this blog I going to show you how you could easily use the Pyspark to wrangle the gigabyte scale data set. OK, so let\u0026rsquo;s get started:\nfrom IPython.core.display import Image Image('start.jpeg', width=1000, height=1000)  Setting up the Docker Engine Now, here the docker will be used to easily download the jupyter/pyspark docker image and then use it for distributed processing. So, the first thing you must know is whether your OS has a Docker engine.","tags":[],"title":"When your Docker Meets Pyspark to Do Sentiment Analysis of 10+ GB Customer Review Data-PART 1","type":"post"},{"authors":["Peyman"],"categories":["readr","readxl","data import"],"content":"  \u0026ndash;\u0026gt;\n -- Ryan Timpe has done great job with his brickr package. Now living in the Denmark, the only think I knew about the Denmark was the LEGO toys :).\nNow thanks to his Brickr package now you can build the Musiac of the your selfie. Fist, I found about this capability from Ryan tweet, here:\nLearn #brickr:\nTurn yourself (or your dog) into a LEGO mosaic in 3 lines of #rstats code. pic.twitter.com/aT0pnYFAo5\n\u0026mdash; Ryan Timpe (@ryantimpe) April 8, 2020  Now, I am going to make my own one :)\nThe first thing you need to do is load the package:\n# If you have not installed the package #install.packages(\u0026#39;brickr\u0026#39;) library(brickr) Then, next step is to read and upload your photo, I am planning to use this photo:\nDownload selfie.png Ok, now you have the selfie.png at your directory, we need to read it through the png:\nawseome_selfie \u0026lt;- png::readPNG(\u0026quot;selfie.png\u0026quot;) Ok, now having that, the next step you must take is some pipe as the below:\nawseome_selfie %\u0026gt;% image_to_mosaic(img_size = 62) %\u0026gt;% build_mosaic() ","date":1586390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586390400,"objectID":"a06b17c0883826f31aaa4ab76153af2c","permalink":"/blog/brickrselfie/","publishdate":"2020-04-09T00:00:00Z","relpermalink":"/blog/brickrselfie/","section":"blog","summary":"Use the Brickr package to use lego toys version of your selfie","tags":null,"title":"Make Musaic Format of Your Selfie","type":"blog"},{"authors":["Peyman"],"categories":["readr","readxl","data import"],"content":"  \u0026ndash;\u0026gt;\n -- Ryan Timpe has done great job with his brickr package. Now living in the Denmark, the only think I knew about the Denmark was the LEGO toys :).\nNow thanks to his Brickr package now you can build the Musiac of the your selfie. Fist, I found about this capability from Ryan tweet, here:\nLearn #brickr:\nTurn yourself (or your dog) into a LEGO mosaic in 3 lines of #rstats code. pic.twitter.com/aT0pnYFAo5\n\u0026mdash; Ryan Timpe (@ryantimpe) April 8, 2020  Now, I am going to make my own one :)\nThe first thing you need to do is load the package:\n# If you have not installed the package #install.packages(\u0026#39;brickr\u0026#39;) library(brickr) Then, next step is to read and upload your photo, I am planning to use this photo:\nDownload selfie.png Ok, now you have the selfie.png at your directory, we need to read it through the png:\nawseome_selfie \u0026lt;- png::readPNG(\u0026quot;selfie.png\u0026quot;) Ok, now having that, the next step you must take is some pipe as the below:\nawseome_selfie %\u0026gt;% image_to_mosaic(img_size = 62) %\u0026gt;% build_mosaic() ","date":1586390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586390400,"objectID":"d963874b4db9d600940b37eac7185a5e","permalink":"/post/brickrselfie/","publishdate":"2020-04-09T00:00:00Z","relpermalink":"/post/brickrselfie/","section":"post","summary":"Use the Brickr package to use lego toys version of your selfie","tags":null,"title":"Make Musaic Format of Your Selfie","type":"post"},{"authors":["Peyman"],"categories":["readr","readxl","data import"],"content":" Question 3.1 Plotting The data set is divided to two sets. From the beginning until the beginning of the 2018 is considered as the Train Data set and from Jan 2018 beyond is considered the Test data-set.\nlibrary(kableExtra) library(tidyverse) library(forecast) library(lubridate) library(car) library(scales) library(patchwork) Data Import: The data will of mounthly Co2 Concentration in (Part Per Million) over the period of the “1960/3-2019/12” measured at Mauna Loa Observatory, Hawaii. The link for the this data available at:\n[ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt]\nNow, to read this data in the R, couples of points must be considered:\n The data has the comments, which is not our interest of analysis. Therefore, in import code we show that with comment.char = ‘#’ , The data has 7 columns yet some of them like the Year and Month label have not been written in the data, so while importing we assing the folloowing column names: The columns wwere seperate using the white space, therfore the sep = ’’ will be added to the code. Year,Month,Time,Co2_con,Interpolated,Trend,Days Since we are including the column names, the header = F could be included.  data \u0026lt;- read.delim(\u0026#39;ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt\u0026#39;, comment.char = \u0026#39;#\u0026#39;, header = F, sep = \u0026#39;\u0026#39;, col.names = c(\u0026#39;Year\u0026#39;,\u0026#39;Month\u0026#39;,\u0026#39;Time\u0026#39;,\u0026#39;Co2_Concentration\u0026#39;,\u0026#39;Interpolated\u0026#39;,\u0026#39;Trend\u0026#39;,\u0026#39;Days\u0026#39;)) Make Data Tidy: Look on any NA values:\nwhich(is.na(data)) # integer(0) Good, we have the complete measured data! However when read the data we see some -99.99 values! - be careful, as was mentioned in the comments these values are when the measurement were not avilable - so for these points (which are 7 out of 741 measueremnets), we use the Interpolated colmns:\ndata_cc \u0026lt;- data %\u0026gt;% mutate( Co2_Con = case_when( Co2_Concentration == -99.99 ~ Interpolated, TRUE ~ Co2_Concentration ) ) Let’s jhave look on column types:\nsapply(data_cc, class) # Year Month Time Co2_Concentration # \u0026quot;integer\u0026quot; \u0026quot;integer\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; # Interpolated Trend Days Co2_Con # \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;integer\u0026quot; \u0026quot;numeric\u0026quot; We can see that column types are in approriate format, yet we can add the new column named Date which give the date of measurement in the standard time sery format:\n  Data Transform Here Lubridate package provides a easy method to convert our Year and Month column to date:\ndata_cc$Date \u0026lt;- ymd(paste0(data$Year, \u0026quot; \u0026quot;, data$Month, \u0026quot; \u0026quot;, \u0026quot;15\u0026quot;)) Also, we can see in the analysis we want to do, we do not the following columsn, so we could select the requered column needed our analys:\ndata_cc_sel \u0026lt;- data_cc %\u0026gt;% select(Year, Month, Date, Co2_Con ) Also, we need to have portion of our data, to test the model we develop based on the training data- So, Here, we consider the data for 2017, 2018 and 2019 as the test data, the rest are the training data.\ndata_cc_sel_test \u0026lt;- data_cc_sel %\u0026gt;% filter(Year \u0026gt; 2016) data_cc_sel_train \u0026lt;- data_cc_sel %\u0026gt;% filter(Year \u0026lt;= 2016)  Data Visulization Now, let’s visulize the data first,\nggplot(data_cc_sel,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;5 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1),axis.title.y = element_text(size = 10)) p1 \u0026lt;- ggplot(data_cc_sel,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;5 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1),axis.title.y = element_text(size = 8)) p2 \u0026lt;- ggplot(data_cc_sel_train,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;5 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1), axis.title.y = element_text(size = 8)) p3 \u0026lt;- ggplot(data_cc_sel_test,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;1 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1), axis.title.y = element_text(size = 8)) (p2 | p3 ) / p1   Modeling: In time series analysis, first things we need to know about the trends are:\n Is the data staionary? Answer: Not, we see the clear trend in in the plot, so the co2 concentration depends on time. (sig of non -stationary) Is there any seasonality in data? Answer: Yes, we can difintely see the seasonality in the data. Now, knowing the the non-statinary and seasonality of the data, it suggest to use theseasons differencing to model the data. To answer, How is Autocorelation function and Partial Auto corellation?  Here is the plot of ACF and PACF from the forecast package:\nCo2_train \u0026lt;- ts(data_cc_sel_train$Co2_Con, start = c(1958,3), frequency = 12) Co2_train %\u0026gt;% ggtsdisplay() Clearly the the data shows the differencing, now we make the ordinary diffrencing of the with the lag of 12:\n#Co2_train %\u0026gt;% diff(1,lag=12) %\u0026gt;% ggtsdisplay() Co2_train %\u0026gt;% diff(lag=12) %\u0026gt;% diff() %\u0026gt;% ggtsdisplay() Now, it is better, we subtanitally removed the trend and as well the ACF is declining until the lag =12. At this stage, we can go for another diffrencing , but it is choice and there is no clear distinction. We start the model with the \\(d=D = 1\\) in the $ARIMA(p,d,q)(P,D,Q)[12] using the forecast package.\nNow, we must have some starting parameters for p,q,D,Q . So, let’s look on the above ACF and PACF: * In the seasonal lags, there is one significant spike in the ACF, suggesting a possible MA(1) term. so, the starting point is \\(Q\\) = 1\n In the plots of the non-seasonal differenced data, there are three spikes at ACF plot, this may be suggestive of a seasonal MA(3) term, \\(q=3\\)  Cosequently, we start withe the \\(ARIMA(0,1,3)(3,1,1)[12]\\) and make variations in the AR and MA terms. Here, while keeping the order constant (d,D), we use the AICs values to judge the quelity of models. (minimize the AICs)\n(fitnew_1 \u0026lt;- Arima(Co2_train, order=c(0,1,3),seasonal=list(order=c(3,1,1),period=12), include.drift = T, lambda = \u0026quot;auto\u0026quot; )) # Series: Co2_train # ARIMA(0,1,3)(3,1,1)[12] # Box Cox transformation: lambda= 0.7524786 # # Coefficients: # ma1 ma2 ma3 sar1 sar2 sar3 sma1 # -0.3653 -0.0298 -0.0744 0.0077 -0.0234 0.0043 -0.8749 # s.e. 0.0389 0.0415 0.0394 0.0460 0.0443 0.0442 0.0253 # # sigma^2 estimated as 0.005423: log likelihood=831.19 # AIC=-1646.38 AICc=-1646.16 BIC=-1610.05 checkresiduals(fitnew_1, lag=36) # # Ljung-Box test # # data: Residuals from ARIMA(0,1,3)(3,1,1)[12] # Q* = 29.875, df = 29, p-value = 0.4203 # # Model df: 7. Total lags used: 36 aicsvalue \u0026lt;- function(p,q,P,Q) { fit \u0026lt;- Arima(Co2_train, order=c(p,1,q),seasonal=list(order=c(P,1,Q),period=12), lambda = \u0026quot;auto\u0026quot; ) return(fit$aicc) } model_eva \u0026lt;- data.frame(Model_name=c(\u0026quot;ARIMA(0,1,3)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(0,1,1)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,2)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,3)(0,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(0,1,1)[12]\u0026quot; ), AICc=c(aicsvalue(0,3,3,1),aicsvalue(0,1,3,1),aicsvalue(1,0,1,0), aicsvalue(1,2,1,0),aicsvalue(1,3,0,1),aicsvalue(1,1,1,0), aicsvalue(1,1,1,0),aicsvalue(1,0,1,1), aicsvalue(1,1,0,1))) Based on the above abalysis, the \\(ARIMA(1,1,1)(0,1,1)[12]\\) will be selected, but we need to check the residual to avoid any over and under fitting as well, to see the Ljung-Box test whether the the residuals resembles white noise or not.\n(fit_minaicc \u0026lt;- Arima(Co2_train, order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12), lambda = \u0026quot;auto\u0026quot; )) # Series: Co2_train # ARIMA(1,1,1)(0,1,1)[12] # Box Cox transformation: lambda= 0.7524786 # # Coefficients: # ar1 ma1 sma1 # 0.2026 -0.5644 -0.8754 # s.e. 0.0935 0.0790 0.0196 # # sigma^2 estimated as 0.005414: log likelihood=829.75 # AIC=-1651.5 AICc=-1651.44 BIC=-1633.34 checkresiduals(fit_minaicc, lag=36) # # Ljung-Box test # # data: Residuals from ARIMA(1,1,1)(0,1,1)[12] # Q* = 34.163, df = 33, p-value = 0.4116 # # Model df: 3. Total lags used: 36 fit_minaicc$aicc # [1] -1651.442 Now, we can see the residualy sufficiently resembles the white noise also the p value high and the model pass the test for Ljong-Box test. (However it must be mentioned the one of the ACF are just reach the boundary in blue line, yet, I do not think it will affect the prediction subtantially - sometime it is difffuclt to have model pass al test.)\nHowever, still this is not the end of model selection. Here, we check the perfomance of the model on the Test data. We ssek to find the model which minime the RMSE.\nCo2_test \u0026lt;- ts(data_cc_sel_test$Co2_Con, start = c(2017,1), frequency = 12) mm \u0026lt;- accuracy(forecast(fit_minaicc,h=35)$mean, Co2_test ) This section compares the RMSE values for the 9 models provided in the previous section.\nrmse_eva \u0026lt;- function(p,d,q,P,D,Q) { fit \u0026lt;- Arima(Co2_train, order=c(p,d,q),seasonal=list(order=c(P,D,Q),period=12), lambda = \u0026quot;auto\u0026quot; ) mm \u0026lt;- accuracy(forecast(fit,h=35)$mean, Co2_test) return(mm[2]) } rmse_eva \u0026lt;- data.frame(Model_name=c( \u0026quot;ARIMA(0,1,3)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(0,1,1)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,2)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,3)(0,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(0,1,1)[12]\u0026quot; ), RMSE=c( rmse_eva(0,1,3,3,1,1),rmse_eva(0,1,1,3,1,1),rmse_eva(1,1,0,1,1,0), rmse_eva(1,1,2,1,1,0),rmse_eva(1,1,3,0,1,1),rmse_eva(1,1,1,1,1,0), rmse_eva(1,1,1,1,1,0),rmse_eva(1,1,0,1,1,1),rmse_eva(1,1,1,0,1,1)))  The results show that the the model \\(ARIMA(1,1,1)(0,1,1)[12]\\) has not the minimum \\(RMSE\\) values, yet it was very close to the minimum, however it was minimum in the \\(AICc\\) values. Atbthe end, knowing that the model residuals foloow the white noise, the \\(RMSE\\) is the final criteria to the selection since it performs on the data the model has not seen in the training process. Using the forecast package, the figures shows the model prediction until the 2050, given the confidence intervals.\nCo2_train %\u0026gt;% Arima(order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12), lambda = \u0026quot;auto\u0026quot; ) %\u0026gt;% forecast(h=400) %\u0026gt;% autoplot() + ylab(\u0026quot;H02 sales (million scripts)\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + autolayer(Co2_test) Let’s zoom in the model prediction and the test data to see the model perfomance visually:\nprediction \u0026lt;- forecast(fit_minaicc,h=39) data_cc_sel_test$prediction \u0026lt;- prediction$mean data_test_pre_tidy \u0026lt;- gather(data_cc_sel_test, \u0026quot;type\u0026quot;, \u0026quot;Co2\u0026quot;, -Year,-Month,-Date) ggplot(data_test_pre_tidy,aes(Date, Co2,color=type)) + geom_line() + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;1 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1), axis.title.y = element_text(size = 8)) Now, given the developed model the question we want to answer is:\nGiven the developed model, what is the chance reaching 460 ppm at 2050? To answer this question , we first need build the cumulative distribution of the Co2 Concentration at the 2050:\nprediction1 \u0026lt;- forecast(fit_minaicc,h=396, level = c(80,90)) p10 \u0026lt;- prediction1$upper[396,2] p50 \u0026lt;- prediction1$mean[396] sd_calc \u0026lt;- (p10-p50)/1.28 Co2_con_2050 \u0026lt;- rnorm(10^6,p50,sd_calc) cdf_co2_con_2050 \u0026lt;- ecdf(Co2_con_2050) cdf_co2_con_2050_data \u0026lt;- data.frame(Co2_con_2050) ggplot(cdf_co2_con_2050_data, aes(Co2_con_2050)) + stat_ecdf(geom = \u0026quot;step\u0026quot;, color=\u0026#39;blue\u0026#39;) + geom_vline(xintercept = 460, color=\u0026#39;red\u0026#39;) + geom_hline(yintercept = cdf_co2_con_2050(460), color=\u0026#39;red\u0026#39;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 0, hjust = 1)) + scale_x_continuous(breaks=c(400,425,450, 460,475,500,525, 550), limits = c(425,525)) + scale_y_continuous(breaks=c(seq(0,1,0.1)), limits = c(0,1)) + ylab(\u0026#39;Cumulative Distribution\u0026#39;) + xlab(\u0026quot;Co2 Concentraion(ppm) at 2050\u0026quot;) Now, having the cumulative distribution, we could ask this question:\n What is the probability the co2 concentraion (ppm) will stay below 460 level by 2050?  cdf_co2_con_2050(460) # [1] 0.088482 As you can see, the answer is around 9%.\n ","date":1586390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586390400,"objectID":"4fa2c9b8cba99ca581212cbc0cd94266","permalink":"/post/timeseries/","publishdate":"2020-04-09T00:00:00Z","relpermalink":"/post/timeseries/","section":"post","summary":"Use Forecast Package for Seasonal ARIMA Modeling","tags":null,"title":"Time Series Modeling for Atmospheric CO2 Concentration(ppm), 1958–2019","type":"post"},{"authors":["Peyman"],"categories":["readr","readxl","data import"],"content":" Question 3.1 Plotting The data set is divided to two sets. From the beginning until the beginning of the 2018 is considered as the Train Data set and from Jan 2018 beyond is considered the Test data-set.\nlibrary(kableExtra) library(tidyverse) library(forecast) library(lubridate) library(car) library(scales) library(patchwork) Data Import: The data will of mounthly Co2 Concentration in (Part Per Million) over the period of the “1960/3-2019/12” measured at Mauna Loa Observatory, Hawaii. The link for the this data available at:\n[ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt]\nNow, to read this data in the R, couples of points must be considered:\n The data has the comments, which is not our interest of analysis. Therefore, in import code we show that with comment.char = ‘#’ , The data has 7 columns yet some of them like the Year and Month label have not been written in the data, so while importing we assing the folloowing column names: The columns wwere seperate using the white space, therfore the sep = ’’ will be added to the code. Year,Month,Time,Co2_con,Interpolated,Trend,Days Since we are including the column names, the header = F could be included.  data \u0026lt;- read.delim(\u0026#39;ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt\u0026#39;, comment.char = \u0026#39;#\u0026#39;, header = F, sep = \u0026#39;\u0026#39;, col.names = c(\u0026#39;Year\u0026#39;,\u0026#39;Month\u0026#39;,\u0026#39;Time\u0026#39;,\u0026#39;Co2_Concentration\u0026#39;,\u0026#39;Interpolated\u0026#39;,\u0026#39;Trend\u0026#39;,\u0026#39;Days\u0026#39;)) Make Data Tidy: Look on any NA values:\nwhich(is.na(data)) # integer(0) Good, we have the complete measured data! However when read the data we see some -99.99 values! - be careful, as was mentioned in the comments these values are when the measurement were not avilable - so for these points (which are 7 out of 741 measueremnets), we use the Interpolated colmns:\ndata_cc \u0026lt;- data %\u0026gt;% mutate( Co2_Con = case_when( Co2_Concentration == -99.99 ~ Interpolated, TRUE ~ Co2_Concentration ) ) Let’s jhave look on column types:\nsapply(data_cc, class) # Year Month Time Co2_Concentration # \u0026quot;integer\u0026quot; \u0026quot;integer\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; # Interpolated Trend Days Co2_Con # \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;integer\u0026quot; \u0026quot;numeric\u0026quot; We can see that column types are in approriate format, yet we can add the new column named Date which give the date of measurement in the standard time sery format:\n  Data Transform Here Lubridate package provides a easy method to convert our Year and Month column to date:\ndata_cc$Date \u0026lt;- ymd(paste0(data$Year, \u0026quot; \u0026quot;, data$Month, \u0026quot; \u0026quot;, \u0026quot;15\u0026quot;)) Also, we can see in the analysis we want to do, we do not the following columsn, so we could select the requered column needed our analys:\ndata_cc_sel \u0026lt;- data_cc %\u0026gt;% select(Year, Month, Date, Co2_Con ) Also, we need to have portion of our data, to test the model we develop based on the training data- So, Here, we consider the data for 2017, 2018 and 2019 as the test data, the rest are the training data.\ndata_cc_sel_test \u0026lt;- data_cc_sel %\u0026gt;% filter(Year \u0026gt; 2016) data_cc_sel_train \u0026lt;- data_cc_sel %\u0026gt;% filter(Year \u0026lt;= 2016)  Data Visulization Now, let’s visulize the data first,\nggplot(data_cc_sel,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;5 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1),axis.title.y = element_text(size = 10)) p1 \u0026lt;- ggplot(data_cc_sel,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;5 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1),axis.title.y = element_text(size = 8)) p2 \u0026lt;- ggplot(data_cc_sel_train,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;5 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1), axis.title.y = element_text(size = 8)) p3 \u0026lt;- ggplot(data_cc_sel_test,aes(Date, Co2_Con)) + geom_line(color=\u0026#39;blue\u0026#39;) + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;1 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1), axis.title.y = element_text(size = 8)) (p2 | p3 ) / p1   Modeling: In time series analysis, first things we need to know about the trends are:\n Is the data staionary? Answer: Not, we see the clear trend in in the plot, so the co2 concentration depends on time. (sig of non -stationary) Is there any seasonality in data? Answer: Yes, we can difintely see the seasonality in the data. Now, knowing the the non-statinary and seasonality of the data, it suggest to use theseasons differencing to model the data. To answer, How is Autocorelation function and Partial Auto corellation?  Here is the plot of ACF and PACF from the forecast package:\nCo2_train \u0026lt;- ts(data_cc_sel_train$Co2_Con, start = c(1958,3), frequency = 12) Co2_train %\u0026gt;% ggtsdisplay() Clearly the the data shows the differencing, now we make the ordinary diffrencing of the with the lag of 12:\n#Co2_train %\u0026gt;% diff(1,lag=12) %\u0026gt;% ggtsdisplay() Co2_train %\u0026gt;% diff(lag=12) %\u0026gt;% diff() %\u0026gt;% ggtsdisplay() Now, it is better, we subtanitally removed the trend and as well the ACF is declining until the lag =12. At this stage, we can go for another diffrencing , but it is choice and there is no clear distinction. We start the model with the \\(d=D = 1\\) in the $ARIMA(p,d,q)(P,D,Q)[12] using the forecast package.\nNow, we must have some starting parameters for p,q,D,Q . So, let’s look on the above ACF and PACF: * In the seasonal lags, there is one significant spike in the ACF, suggesting a possible MA(1) term. so, the starting point is \\(Q\\) = 1\n In the plots of the non-seasonal differenced data, there are three spikes at ACF plot, this may be suggestive of a seasonal MA(3) term, \\(q=3\\)  Cosequently, we start withe the \\(ARIMA(0,1,3)(3,1,1)[12]\\) and make variations in the AR and MA terms. Here, while keeping the order constant (d,D), we use the AICs values to judge the quelity of models. (minimize the AICs)\n(fitnew_1 \u0026lt;- Arima(Co2_train, order=c(0,1,3),seasonal=list(order=c(3,1,1),period=12), include.drift = T, lambda = \u0026quot;auto\u0026quot; )) # Series: Co2_train # ARIMA(0,1,3)(3,1,1)[12] # Box Cox transformation: lambda= 0.7524786 # # Coefficients: # ma1 ma2 ma3 sar1 sar2 sar3 sma1 # -0.3653 -0.0298 -0.0744 0.0077 -0.0234 0.0043 -0.8749 # s.e. 0.0389 0.0415 0.0394 0.0460 0.0443 0.0442 0.0253 # # sigma^2 estimated as 0.005423: log likelihood=831.19 # AIC=-1646.38 AICc=-1646.16 BIC=-1610.05 checkresiduals(fitnew_1, lag=36) # # Ljung-Box test # # data: Residuals from ARIMA(0,1,3)(3,1,1)[12] # Q* = 29.875, df = 29, p-value = 0.4203 # # Model df: 7. Total lags used: 36 aicsvalue \u0026lt;- function(p,q,P,Q) { fit \u0026lt;- Arima(Co2_train, order=c(p,1,q),seasonal=list(order=c(P,1,Q),period=12), lambda = \u0026quot;auto\u0026quot; ) return(fit$aicc) } model_eva \u0026lt;- data.frame(Model_name=c(\u0026quot;ARIMA(0,1,3)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(0,1,1)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,2)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,3)(0,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(0,1,1)[12]\u0026quot; ), AICc=c(aicsvalue(0,3,3,1),aicsvalue(0,1,3,1),aicsvalue(1,0,1,0), aicsvalue(1,2,1,0),aicsvalue(1,3,0,1),aicsvalue(1,1,1,0), aicsvalue(1,1,1,0),aicsvalue(1,0,1,1), aicsvalue(1,1,0,1))) Based on the above abalysis, the \\(ARIMA(1,1,1)(0,1,1)[12]\\) will be selected, but we need to check the residual to avoid any over and under fitting as well, to see the Ljung-Box test whether the the residuals resembles white noise or not.\n(fit_minaicc \u0026lt;- Arima(Co2_train, order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12), lambda = \u0026quot;auto\u0026quot; )) # Series: Co2_train # ARIMA(1,1,1)(0,1,1)[12] # Box Cox transformation: lambda= 0.7524786 # # Coefficients: # ar1 ma1 sma1 # 0.2026 -0.5644 -0.8754 # s.e. 0.0935 0.0790 0.0196 # # sigma^2 estimated as 0.005414: log likelihood=829.75 # AIC=-1651.5 AICc=-1651.44 BIC=-1633.34 checkresiduals(fit_minaicc, lag=36) # # Ljung-Box test # # data: Residuals from ARIMA(1,1,1)(0,1,1)[12] # Q* = 34.163, df = 33, p-value = 0.4116 # # Model df: 3. Total lags used: 36 fit_minaicc$aicc # [1] -1651.442 Now, we can see the residualy sufficiently resembles the white noise also the p value high and the model pass the test for Ljong-Box test. (However it must be mentioned the one of the ACF are just reach the boundary in blue line, yet, I do not think it will affect the prediction subtantially - sometime it is difffuclt to have model pass al test.)\nHowever, still this is not the end of model selection. Here, we check the perfomance of the model on the Test data. We ssek to find the model which minime the RMSE.\nCo2_test \u0026lt;- ts(data_cc_sel_test$Co2_Con, start = c(2017,1), frequency = 12) mm \u0026lt;- accuracy(forecast(fit_minaicc,h=35)$mean, Co2_test ) This section compares the RMSE values for the 9 models provided in the previous section.\nrmse_eva \u0026lt;- function(p,d,q,P,D,Q) { fit \u0026lt;- Arima(Co2_train, order=c(p,d,q),seasonal=list(order=c(P,D,Q),period=12), lambda = \u0026quot;auto\u0026quot; ) mm \u0026lt;- accuracy(forecast(fit,h=35)$mean, Co2_test) return(mm[2]) } rmse_eva \u0026lt;- data.frame(Model_name=c( \u0026quot;ARIMA(0,1,3)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(0,1,1)(3,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,2)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,3)(0,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;, \u0026quot;ARIMA(1,1,1)(1,1,0)[12]\u0026quot;,\u0026quot;ARIMA(1,1,0)(1,1,1)[12]\u0026quot;,\u0026quot;ARIMA(1,1,1)(0,1,1)[12]\u0026quot; ), RMSE=c( rmse_eva(0,1,3,3,1,1),rmse_eva(0,1,1,3,1,1),rmse_eva(1,1,0,1,1,0), rmse_eva(1,1,2,1,1,0),rmse_eva(1,1,3,0,1,1),rmse_eva(1,1,1,1,1,0), rmse_eva(1,1,1,1,1,0),rmse_eva(1,1,0,1,1,1),rmse_eva(1,1,1,0,1,1)))  The results show that the the model \\(ARIMA(1,1,1)(0,1,1)[12]\\) has not the minimum \\(RMSE\\) values, yet it was very close to the minimum, however it was minimum in the \\(AICc\\) values. Atbthe end, knowing that the model residuals foloow the white noise, the \\(RMSE\\) is the final criteria to the selection since it performs on the data the model has not seen in the training process. Using the forecast package, the figures shows the model prediction until the 2050, given the confidence intervals.\nCo2_train %\u0026gt;% Arima(order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12), lambda = \u0026quot;auto\u0026quot; ) %\u0026gt;% forecast(h=400) %\u0026gt;% autoplot() + ylab(\u0026quot;H02 sales (million scripts)\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + autolayer(Co2_test) Let’s zoom in the model prediction and the test data to see the model perfomance visually:\nprediction \u0026lt;- forecast(fit_minaicc,h=39) data_cc_sel_test$prediction \u0026lt;- prediction$mean data_test_pre_tidy \u0026lt;- gather(data_cc_sel_test, \u0026quot;type\u0026quot;, \u0026quot;Co2\u0026quot;, -Year,-Month,-Date) ggplot(data_test_pre_tidy,aes(Date, Co2,color=type)) + geom_line() + xlab(\u0026quot;Year, Month\u0026quot;) + scale_x_date(date_labels = \u0026quot;%Y-%m\u0026quot;, date_breaks = \u0026quot;1 year\u0026quot;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 45, hjust = 1)) + ylab(\u0026quot;CO2 Concentration (ppm)\u0026quot;) + #scale_x_continuous(breaks = trans_breaks(identity, identity, n = 10)) scale_y_continuous() + theme(axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 10, hjust = 1), axis.title.y = element_text(size = 8)) Now, given the developed model the question we want to answer is:\nGiven the developed model, what is the chance reaching 460 ppm at 2050? To answer this question , we first need build the cumulative distribution of the Co2 Concentration at the 2050:\nprediction1 \u0026lt;- forecast(fit_minaicc,h=396, level = c(80,90)) p10 \u0026lt;- prediction1$upper[396,2] p50 \u0026lt;- prediction1$mean[396] sd_calc \u0026lt;- (p10-p50)/1.28 Co2_con_2050 \u0026lt;- rnorm(10^6,p50,sd_calc) cdf_co2_con_2050 \u0026lt;- ecdf(Co2_con_2050) cdf_co2_con_2050_data \u0026lt;- data.frame(Co2_con_2050) ggplot(cdf_co2_con_2050_data, aes(Co2_con_2050)) + stat_ecdf(geom = \u0026quot;step\u0026quot;, color=\u0026#39;blue\u0026#39;) + geom_vline(xintercept = 460, color=\u0026#39;red\u0026#39;) + geom_hline(yintercept = cdf_co2_con_2050(460), color=\u0026#39;red\u0026#39;) + theme(axis.text.x = element_text(face = \u0026quot;bold\u0026quot;, color = \u0026quot;#993333\u0026quot;, size = 12, angle = 0, hjust = 1)) + scale_x_continuous(breaks=c(400,425,450, 460,475,500,525, 550), limits = c(425,525)) + scale_y_continuous(breaks=c(seq(0,1,0.1)), limits = c(0,1)) + ylab(\u0026#39;Cumulative Distribution\u0026#39;) + xlab(\u0026quot;Co2 Concentraion(ppm) at 2050\u0026quot;) Now, having the cumulative distribution, we could ask this question:\n What is the probability the co2 concentraion (ppm) will stay below 460 level by 2050?  cdf_co2_con_2050(460) # [1] 0.088482 As you can see, the answer is around 9%.\n ","date":1586390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586390400,"objectID":"b9adf47db7a355148a7be312371db21c","permalink":"/project/timeseries/","publishdate":"2020-04-09T00:00:00Z","relpermalink":"/project/timeseries/","section":"project","summary":"Use Forecast Package for Seasonal ARIMA Modeling","tags":null,"title":"Time Series Modeling for Atmospheric CO2 Concentration(ppm), 1958–2019","type":"project"},{"authors":["Peyman Kor"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Peyman Kor","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Peyman Kor","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]